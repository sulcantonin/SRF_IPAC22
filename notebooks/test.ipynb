{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep One-Class Classification\n",
    "http://proceedings.mlr.press/v80/ruff18a/ruff18a.pdf\n",
    "\n",
    "Deep Semi-Supervised Anoamly Detection\n",
    "https://arxiv.org/pdf/1906.02694.pdf\n",
    "\n",
    "\n",
    "(Supervised) Contrastive Loss\n",
    "\n",
    "CSI: Novelty Detection via Contrastive Learningon Distributionally Shifted Instances : https://arxiv.org/pdf/2007.08176.pdf\n",
    "\n",
    "A Unifying Review of Deep and Shallow AnomalyDetection : https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347460&tag=1\n",
    "\n",
    "Dropout techniques on RNNs\n",
    "\n",
    "https://adriangcoder.medium.com/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "from openTSNE import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from math import ceil\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "sys.path.insert(0, '../code/')\n",
    "sys.path.insert(0, './models/')\n",
    "import util\n",
    "from models import *\n",
    "import dataset\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import *\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "\n",
    "t2n = lambda x : x.cpu().detach().numpy()\n",
    "\n",
    "parameters = {\n",
    "    'input_dims' : [0,1,2,3,4,5],\n",
    "    # 'input_dims' : [0,2,4],\n",
    "    'num_epochs' : 128,\n",
    "    'learning_rate' : 0.001,\n",
    "    'weight_decay' : 10e-4,\n",
    "    'eta_anom' : 1.0,\n",
    "    'eta_label' : 5.0,\n",
    "    'eta_network' : 1/2,\n",
    "    'n_batches' : 32,\n",
    "    'test_batches': 3,\n",
    "    'num_hidden' : 256,\n",
    "    'num_hidden_layers' : 1}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "#\n",
    "# --- NETWORK & OPTIM ---\n",
    "#\n",
    "\n",
    "\n",
    "# regular\n",
    "model = QuenchDetectionNetworkLSTM(parameters['num_hidden'],\n",
    "                                   parameters['num_hidden_layers'],\n",
    "                                   parameters['eta_anom'], \n",
    "                                   parameters['eta_label'], \n",
    "                                   parameters['eta_network'],\n",
    "                                   len(parameters['input_dims'])).to(device)\n",
    "# folder = '../models/LSTM_256_LSTM2_Linear1/eta_anom_1_eta_label_5/'\n",
    "# folder = '../models/LSTM_256_LSTM2_Linear1/amplitude_eta_anom_1_eta_label_5/'\n",
    "# folder = '../models/LSTM_256_LSTM2_Linear1/only_healthly_eta_anom_1_eta_label_5/'\n",
    "# folder = '../models/LSTM_256_LSTM2_Linear1/eta_anom_1_eta_label_5_32batches/'\n",
    "# folder = '../models/LSTM_256_LSTM1_Linear1/eta_anom_1_eta_label_5_jan22/'\n",
    "\n",
    "#\n",
    "# from IPAC, but enable biases! \n",
    "#\n",
    "# print('from IPAC, but enable biases!')\n",
    "folder = '../models/LSTM_256_LSTM1_Linear1/eta_anom_1_eta_label_5_feb22_2/'  \n",
    "# model.c = model.c.to(device)\n",
    "# folder = '../models/LSTM_256_LSTM1_Linear1/eta_anom_1_eta_label_5_feb22_nobias_const_c/'\n",
    "\n",
    "# linear beteween\n",
    "'''\n",
    "model = QuenchDetectionNetworkLSTM2LinearBetween(num_hidden = parameters['num_hidden'],\n",
    "                                   num_hidden_layers = parameters['num_hidden_layers'],\n",
    "                                   eta_anom = parameters['eta_anom'], \n",
    "                                   eta_label = parameters['eta_label'], \n",
    "                                   eta_network = parameters['eta_network']).to(device)\n",
    "folder = '../models/LSTM_256_LSTM2_Linear1_LinearBetween/eta_anom_1_eta_label_5/'\n",
    "'''\n",
    "\n",
    "# layer norm.  \n",
    "'''\n",
    "model = QuenchDetectionNetworkLSTM2LayerNorm(num_hidden = parameters['num_hidden'],\n",
    "                                   num_hidden_layers = parameters['num_hidden_layers'],\n",
    "                                   eta_anom = parameters['eta_anom'], \n",
    "                                   eta_label = parameters['eta_label'], \n",
    "                                   eta_network = parameters['eta_network']).to(device)\n",
    "folder = '../models/LSTM_256_LSTM2_Linear1_LayerNorm/eta_anom_1_eta_label_5/'\n",
    "'''\n",
    "'''\n",
    "# softmax loss - i.e. a regular classifier\n",
    "model = QuenchDetectionNetworkClassifierLSTM(num_hidden = parameters['num_hidden'],\n",
    "                                   num_hidden_layers = parameters['num_hidden_layers'],\n",
    "                                   eta_anom = parameters['eta_anom'], \n",
    "                                   eta_label = parameters['eta_label'], \n",
    "                                   eta_network = parameters['eta_network']).to(device)\n",
    "folder = '../models/LSTM_256_LSTM1_Linear1_Classifier/eta_anom_1_eta_label_5_2(same as OC)_2/'\n",
    "'''\n",
    "if not os.path.exists(folder):\n",
    "    print(f'creating {folder}')\n",
    "    os.makedirs(folder)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=parameters['learning_rate'], weight_decay = parameters['weight_decay'])\n",
    "print(model)\n",
    "last_epoch = 0\n",
    "\n",
    "weights_file = sorted(glob(folder + 'anomaly_epoch_0*'))[-1]\n",
    "print(f'last epoch {last_epoch} from file {weights_file}')\n",
    "last_epoch = int(weights_file.split('_')[-1])\n",
    "model.load_state_dict(torch.load(weights_file,map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_labels = {0 : 'Quench', \n",
    "             4 : 'Jumps in probe', \n",
    "             5 : 'Time misalignment', \n",
    "             6 : 'Ringing', \n",
    "             7 : 'Pulse Cut', \n",
    "             8 :'HW C7M1A14 --> HW has been fixed now',\n",
    "             10 : 'Other Anomaly',\n",
    "             11 : 'Beam'}\n",
    "\n",
    "df = pd.concat((pd.read_csv('/home/sulcan/data/quench/testRun2020_results.csv'),\n",
    "                pd.read_csv('/home/sulcan/data/quench/testRun2021_results.csv'),\n",
    "                pd.read_csv('/home/sulcan/data/quench/testRun2021_results_classify.csv')))\n",
    "\n",
    "df['file'] = [file.split('/')[-1] for file in df['file']]\n",
    "\n",
    "def get_labels(df, file, location,var):\n",
    "    df_ = df[np.bitwise_and(df['file'] == file,df['location'] == location)][var]\n",
    "    df_ = df_[~pd.isna(df_)]\n",
    "    return list(df_.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Traning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_folders = ['/home/sulcan/data/quench/down_labelled/']\n",
    "faulty_files = ['/home/sulcan/data/quench/X_down_labelled_all.pickle']\n",
    "\n",
    "\n",
    "healthly_files =   ['/home/sulcan/data/quench/X_up_2021-08_all.pickle',\n",
    "                    '/home/sulcan/data/quench/X_up_2021-10_all.pickle', \n",
    "                    '/home/sulcan/data/quench/X_up_2021-11_all.pickle',\n",
    "                    '/home/sulcan/data/quench/X_up_2022-01_all.pickle',\n",
    "                     '/home/sulcan/data/quench/X_up_2022-02_all.pickle'\n",
    "                   ]\n",
    "\n",
    "healthly_folders = ['/home/sulcan/data/quench/up_filtered/2021-08-*',\n",
    "                    '/home/sulcan/data/quench/up_filtered/2021-10-*',\n",
    "                    '/home/sulcan/data/quench/up_filtered/2021-11-*',\n",
    "                    '/home/sulcan/data/quench/up_jan/X_up_2022-01-*',\n",
    "                    '/home/sulcan/data/quench/up_feb/X_up_2022-02-*',\n",
    "                   ]\n",
    "\n",
    "X_faulty, files_and_locations_faulty, pids_faulty = dataset.load_multiple_data_from_numpy(faulty_files, faulty_folders, parameters['input_dims'])\n",
    "X_healthly, files_and_locations_healthly, pids_healthly = dataset.load_multiple_data_from_numpy(healthly_files, healthly_folders, parameters['input_dims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- putting everything together into an input----\n",
    "input_raw = X_healthly + X_faulty\n",
    "files_and_locations = files_and_locations_healthly + files_and_locations_faulty\n",
    "pids = pids_healthly + pids_faulty\n",
    "# (A,P) -> (I,Q)\n",
    "if len(parameters['input_dims']) == 6: \n",
    "    input = dataset.transform_RF_pulses(deepcopy(input_raw))\n",
    "# just selecting subset of values\n",
    "elif len(parameters['input_dims']) == 3:\n",
    "    input = dataset.select_dimensions(deepcopy(input_raw),parameters['input_dims'])\n",
    "input = dataset.normalize_data(input)\n",
    "\n",
    "# --- creating labels ----\n",
    "labels = [+1] * len(X_healthly) + [-1] * len(X_faulty)\n",
    "\n",
    "print(f'healthly {len(X_healthly)} faulty {len(X_faulty)}')\n",
    "\n",
    "# del X_faulty\n",
    "# del X_healthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation for batch indices\n",
    "if os.path.isfile(folder + '/permutation.npy'):\n",
    "    print('loading permutation')\n",
    "    permutation = np.load(folder + '/permutation.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # file = '/home/sulcan/data/quench/X_down_labelled_classified_all.pickle' # \n",
    "    # file = '/home/sulcan/data/quench/X_down_labelled_all.pickle' # \n",
    "    # file = '/home/sulcan/data/quench/X_down_all.pickle' # \n",
    "    file = '/home/sulcan/data/quench/X_up_2021-08_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2021-10_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2021-11_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-01_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-02_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-03_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-04_all.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-mai_cryo_all_03am.pickle'\n",
    "    # file = '/home/sulcan/data/quench/X_up_2022-mai_cryo_all_114726.pickle'\n",
    "    \n",
    "\n",
    "    input_raw_, files_and_locations_, pids_ = dataset.load_data_from_numpy([], file, parameters['input_dims'])\n",
    "\n",
    "    crit = lambda x : x.shape[-1] > 50\n",
    "    keep = np.where(np.array(list(map(crit, input_raw_))))[0]\n",
    "    input_raw_ = [input_raw_[i] for i in keep]\n",
    "    files_and_locations_ = [files_and_locations_[i] for i in keep]\n",
    "\n",
    "    skip = 1\n",
    "    input_ = deepcopy(input_raw_[::skip])\n",
    "    input_raw_ = input_raw_[::skip]\n",
    "\n",
    "    # (A,P) -> (I,Q)\n",
    "    if len(parameters['input_dims']) == 6: \n",
    "        input_ = dataset.transform_RF_pulses(input_)\n",
    "    # just selecting subset of values\n",
    "    elif len(parameters['input_dims']) == 3:\n",
    "        input_ = dataset.select_dimensions(input_,parameters['input_dims'])\n",
    "    input_ = dataset.normalize_data(input_)\n",
    "else:\n",
    "    # copying data from a permutation[0] in training set\n",
    "    indices = permutation[0] + permutation[1] + permutation[2]\n",
    "    input_ = [input[i] for i in indices]\n",
    "    input_raw_ = [input_raw[i] for i in indices]\n",
    "    files_and_locations_ = [files_and_locations [i] for i in indices]\n",
    "    pids_ = [pids[i] for i in indices]\n",
    "    L = [labels[i] for i in indices]\n",
    "    N = len(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = [model(torch.cat([input_[i]] * 1).to(device)) for i in tqdm(range(len(input_)))]\n",
    "    # y = torch.cat([y[i][1][-1] for i in range(len(y))])\n",
    "    # phi = [model.phi(torch.cat([input_[i]] * 1).to(device)) for i in tqdm(range(len(input_)))]\n",
    "    # phi = torch.cat([phi_[-1:,0,...] for phi_ in phi],0)\n",
    "model.train()\n",
    "y_ = torch.cat([y[i][1][-1] for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test data (excluded from training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [6,2]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.hist(t2n(y_).ravel(),64, color='green')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(fontsize = 14)\n",
    "# plt.savefig('/home/sulcan/TUPOPT062_10.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = (y_.mean() + 2 * y_.std()).item()\n",
    "sum(y_ < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = torch.cat([y[i][1][-1] for i in range(len(y))])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 6]\n",
    "\n",
    "\n",
    "plt.plot(t2n(y_),linewidth = 0.5)\n",
    "plt.plot(L,'--',linewidth = 0.5)\n",
    "plt.legend(['Anomaly Score of Last Pulse','Label (-1 : F, 1 H)'])\n",
    "plt.show()\n",
    "\n",
    "T = (y_.mean() + 2 * y_.std()).item()\n",
    "#T = 0.0 # 0.6704126596450806\n",
    "# CONFUSIO MATRIX\n",
    "C = pd.DataFrame(confusion_matrix(list(map(lambda l : 0 if l == 1 else 1,L)), t2n(y_) > T), index = ['Actual H','Actual F'], columns = ['Pred H','Pred F'])\n",
    "print(C)\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(list(map(lambda l : 0 if l == 1 else 1,L)), t2n(y_))\n",
    "auc_score = auc(fpr.astype(np.float32),tpr.astype(np.float32))\n",
    "plt.subplot(121)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.title(f'ROC Curve (AUC {auc_score})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Postitive Rate')\n",
    "\n",
    "print(sum(y_ <= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 200 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams['text.usetex'] = True\n",
    "N = len(y)\n",
    "T = 0.0\n",
    "\n",
    "# order = np.random.permutation(len(y))\n",
    "# order = np.where(np.array(L) == -1)[0]\n",
    "# N = np.min([len(order), N])\n",
    "\n",
    "y_ = torch.stack([y[i][1][-1,0,0] for i in range(len(y))]) # last \n",
    "order = np.argsort(t2n(y_).ravel())[::1]\n",
    "order = range(len(y_))\n",
    "# order = [i[0] for i in sorted(enumerate(locations), key=lambda x:x[1])]\n",
    "# order = order[200:]\n",
    "\n",
    "\n",
    "'''\n",
    "plt.rcParams['figure.figsize'] = [6,2]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.hist(t2n(y_).ravel(),64, color='red')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.show()\n",
    "# plt.savefig('/home/sulcan/hist_LAB.pdf')\n",
    "'''\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 2]\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    i = order[i]\n",
    "\n",
    "    x = input_raw_[i]\n",
    "    # x = input_[i].reshape((-1,6,182)).permute((1,2,0))\n",
    "    s = y[i][1].cpu().ravel()\n",
    "    filename = files_and_locations_[i][0].split('/')[-1]\n",
    "    location = files_and_locations_[i][1]\n",
    "    \n",
    "    \n",
    "    # getting \"GT\" labels\n",
    "    gt_anomaly = get_labels(df,filename, location, 'anomaly')\n",
    "    gt_classify_ae = get_labels(df,filename, location, 'classify_ae')\n",
    "    \n",
    "    '''\n",
    "    if gt_classify_ae[0] != 0:\n",
    "        continue\n",
    "    '''\n",
    "    '''\n",
    "    if len(gt_classify_ae) == 0:\n",
    "        continue\n",
    "    '''\n",
    "    gt_classify_ae = (ae_labels[gt_classify_ae[0]] if len(gt_classify_ae) > 0 else 'NA')\n",
    "    \n",
    "    # ampltidue\n",
    "    plt.subplot(1,2,(1))\n",
    "    plt.plot(x[0,:,:],'r-')\n",
    "    plt.plot(x[2,:,:],'g-')\n",
    "    plt.plot(x[4,:,:],'b-')\n",
    "    plt.xlabel('Time [$\\mu$s]')\n",
    "    plt.ylabel('Field amplitude [MV/m]')\n",
    "    plt.xticks(range(0,x.shape[1],25),10 * np.arange(0,x.shape[1],25))    \n",
    "\n",
    "    '''\n",
    "    # phase\n",
    "    plt.subplot(3,2,2)\n",
    "    plt.plot(x[1,:,:],'r-')\n",
    "    plt.plot(x[3,:,:],'g-')\n",
    "    plt.plot(x[5,:,:],'b-')\n",
    "    plt.title(f'anomaly(label) {gt_anomaly}\\nclassify ae (label) {gt_classify_ae}')\n",
    "    '''\n",
    "    \n",
    "    plt.subplot(1,2,(2))\n",
    "    plt.plot(s[10:])\n",
    "    plt.xlabel('Pulse')\n",
    "    plt.ylabel('Anomaly score $s$')\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    \n",
    "    print('-'.join([filename, location]))\n",
    "    plt.savefig('/home/sulcan/images/q/02/' + filename + '_' + location + '_' + gt_classify_ae + '.pdf')\n",
    "\n",
    "    '''\n",
    "    plt.subplot(3,2,5)\n",
    "    grad = show_input_gradients_for_output(input_[i],model, device, len(parameters['input_dims']))\n",
    "    plt.plot(np.abs(grad[-1,0,:]),'r-')\n",
    "    plt.plot(np.abs(grad[-1,1,:]),'g-')\n",
    "    plt.plot(np.abs(grad[-1,2,:]),'b-')    \n",
    "    plt.title('input grad for I')\n",
    "    \n",
    "    plt.subplot(3,2,6)\n",
    "    grad = show_input_gradients_for_output(input_[i],model,device, len(parameters['input_dims']))\n",
    "    plt.plot(np.abs(grad[-1,1,:]),'r-')\n",
    "    plt.plot(np.abs(grad[-1,3,:]),'g-')\n",
    "    plt.plot(np.abs(grad[-1,5,:]),'b-')\n",
    "    plt.title('input grad for Q')\n",
    "    '''\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "locations = []\n",
    "\n",
    "for i,fa in enumerate(files_and_locations_):\n",
    "    expr = re.findall('^C*(.*)M(.*)A(.*)L(.*)$', fa[1])[0]\n",
    "    expr = 'L' + str(int(float(expr[3]))).zfill(2) + 'A' + str(int(float(expr[2]))).zfill(2) + 'M' + str(int(float(expr[1]))).zfill(2) + 'C' + str(int(float(expr[0]))).zfill(2)\n",
    "    locations.append(expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.autolayout'] = True\n",
    "order = [i[0] for i in sorted(enumerate(locations), key=lambda x:x[1])]\n",
    "plt.rcParams['figure.figsize'] = [50,2]\n",
    "plt.plot([locations[i] for i in order],t2n(y_[order]))\n",
    "plt.plot(t2n(y_[order]))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.savefig('/home/sulcan/scores_interlock.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(columns = ['file', 'location', 'pid', 'anomaly_score_rnn_as',\\\n",
    "                                    'anomaly_threshold_rnn', 'model_file', 'anomaly_ae', 'classify_ae'])\n",
    "T = 0.0\n",
    "\n",
    "rows = []\n",
    "for i in tqdm(range(0,N,1)):\n",
    "    i = order[i]\n",
    "    \n",
    "    scores = t2n(y[i][1][:]).ravel()\n",
    "    anomaly = scores >= T\n",
    "    \n",
    "    filename = files_and_locations_[i][0].split('/')[-1]\n",
    "    location = files_and_locations_[i][1]\n",
    "    pid = pids_[i]\n",
    "          \n",
    "    pid = pid if len(pid) == len(scores) else [-1] * len(scores)\n",
    "    \n",
    "    for j in np.where(anomaly)[0]:\n",
    "    \n",
    "        score = scores[j]\n",
    "        \n",
    "        gt_anomaly = get_labels(df,filename, location, 'anomaly')\n",
    "        gt_classify_ae = get_labels(df,filename, location, 'classify_ae')\n",
    "                \n",
    "        rows +=  [pd.Series(data = {'file' : \"/home/xfeloper/data/MGTF/XTLReport/daq/\" + filename , \\\n",
    "               'location' : str(location), \\\n",
    "               'pid' : int(pid[j]),\\\n",
    "               'anomaly_score_rnn_as' : float(score), \\\n",
    "               'anomaly_threshold_rnn' : float(T),\\\n",
    "               'model_file' : str(weights_file),\\\n",
    "               'anomaly_ae' : str((gt_anomaly[0] if len(gt_anomaly) > 0 else 'NA')),\\\n",
    "               'classify_ae' : str((gt_classify_ae[0] if len(gt_classify_ae) > 0 else 'NA'))}, name = i)]\n",
    "\n",
    "df_result = df_result.append(rows,ignore_index = False)\n",
    "df_result.to_csv('../results/test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs = 32, verbose = True)\n",
    "phit = tsne.fit(t2n(phi))\n",
    "# phit = tsne.fit(t2n(torch.cat((phi_),0)))\n",
    "# phit = tsne.fit(t2n(torch.cat((phif[::2,:],model.c.view((1,-1))),0)))\n",
    "# phit = tsne.fit(t2n(torch.cat((phi03, phi01, phi02),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "phit = np.array(phit)\n",
    "\n",
    "if True:\n",
    "    \n",
    "    N1 = phi03.shape[0]\n",
    "    N2 = phi01.shape[0]\n",
    "    plt.plot(phit[:N1,0],phit[:N1,1],'r.',markersize = 1)\n",
    "    plt.plot(phit[N1+N2:,0],phit[N1+N2:,1],'b.',markersize = 1)\n",
    "    plt.plot(phit[N1:N1+N2,0],phit[N1:N1+N2,1],'g.',markersize = 1)\n",
    "    \n",
    "    '''\n",
    "    N = phil.shape[0]\n",
    "    plt.plot(phit[N:,0],phit[N:,1],'b.',markersize = 1)\n",
    "    plt.plot(phit[:N,0],phit[:N,1],'r.',markersize = 1)\n",
    "    '''\n",
    "    # plt.title('healthly Feb 2022 (blue), Jan 2022 (red)')\n",
    "else: \n",
    "    # phif.shape\n",
    "    F = np.where(np.array(labels)[indices] == -1)[0]\n",
    "    H = np.where(np.array(labels)[indices] == 1)[0]\n",
    "\n",
    "    plt.plot(phit[H,0],phit[H,1],'b.')\n",
    "    plt.plot(phit[F,0],phit[F,1],'r.')\n",
    "    \n",
    "    plt.xlabel('Embedding Dimension 1')\n",
    "    plt.ylabel('Embedding Dimension 2')\n",
    "    plt.title()    \n",
    "plt.savefig('/home/sulcan/TSNE_MAR_JAN_FEB.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE embedding of classified data \n",
    "N = len(input_)\n",
    "L = []\n",
    "for i in range(N):\n",
    "    filename = files_and_locations_[i][0].split('/')[-1]\n",
    "    location = files_and_locations_[i][1]\n",
    "    gt_classify_ae = get_labels(df,filename, location, 'classify_ae')\n",
    "    \n",
    "    L.append(gt_classify_ae[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs = 32, verbose = True)\n",
    "phit = tsne.fit(t2n(phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "plt.scatter(phit[:,0],phit[:,1],c = L,cmap ='gist_rainbow',s=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
